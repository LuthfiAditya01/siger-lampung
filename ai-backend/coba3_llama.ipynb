{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f6cbd0",
   "metadata": {},
   "source": [
    "# coba 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f668826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a61cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Konfigurasi ===\n",
    "FOLDER_DOCS = \"../bahan-chatbot/txt/\"\n",
    "INDEX_PATH = \"vectorstore_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d7dd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M.S.I\\AppData\\Local\\Temp\\ipykernel_20644\\1798122027.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  model = ChatOllama(model=\"gemma:2b\")  # ganti sesuai model yang ada\n",
      "C:\\Users\\M.S.I\\AppData\\Local\\Temp\\ipykernel_20644\\1798122027.py:3: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")  # lebih cepat\n"
     ]
    }
   ],
   "source": [
    "# Pakai model LLM & embedding ringan\n",
    "model = ChatOllama(model=\"gemma:2b\")  # ganti sesuai model yang ada\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")  # lebih cepat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7da27691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fungsi membuat index FAISS ===\n",
    "def build_vectorstore():\n",
    "    docs = []\n",
    "    print(f\"üìÇ Membaca dokumen dari: {FOLDER_DOCS}\")\n",
    "    files = [f for f in os.listdir(FOLDER_DOCS) if f.endswith(\".txt\")]\n",
    "\n",
    "    for filename in tqdm(files, desc=\"üìÑ Membaca file TXT\", unit=\"file\"):\n",
    "        with open(os.path.join(FOLDER_DOCS, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            docs.append(Document(page_content=text, metadata={\"source\": filename}))\n",
    "\n",
    "    print(\"\\n‚úÇÔ∏è  Memotong dokumen menjadi chunks...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "    docs_split = []\n",
    "    for doc in tqdm(docs, desc=\"üîπ Memotong teks\", unit=\"dokumen\"):\n",
    "        chunks = splitter.split_documents([doc])\n",
    "        docs_split.extend(chunks)\n",
    "\n",
    "    print(\"\\nüíæ Membuat FAISS index...\")\n",
    "    vectorstore = FAISS.from_documents(docs_split, embeddings)\n",
    "    vectorstore.save_local(INDEX_PATH)\n",
    "    print(\"‚úÖ Index berhasil dibuat & disimpan!\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68fec5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Memuat FAISS index dari vectorstore_index...\n",
      "‚úÖ Index berhasil dimuat!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Load atau buat index ===\n",
    "if os.path.exists(INDEX_PATH):\n",
    "    print(f\"‚ö° Memuat FAISS index dari {INDEX_PATH}...\")\n",
    "    vectorstore = FAISS.load_local(INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(\"‚úÖ Index berhasil dimuat!\\n\")\n",
    "else:\n",
    "    vectorstore = build_vectorstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94f350cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fungsi chatbot ===\n",
    "def chatbot(query):\n",
    "    results = vectorstore.similarity_search(query, k=1)\n",
    "    \n",
    "    if results:\n",
    "        context = results[0].page_content\n",
    "        prompt = f\"\"\"\n",
    "        Jawab pertanyaan berikut dengan bahasa Indonesia yang jelas.\n",
    "        Jika jawabannya tidak ada di KONTEN, jawab saja: \"Maaf, saya tidak menemukan informasi tersebut di dokumen.\"\n",
    "        Jika relevan, hanya gunakan informasi dari konteks:\n",
    "        KONTEN: {context}\n",
    "        PERTANYAAN: {query}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "        Kamu adalah asisten AI yang ramah dan menjawab dalam bahasa Indonesia.\n",
    "        Pertanyaan: {query}\n",
    "        \"\"\"\n",
    "\n",
    "    response = model([HumanMessage(content=prompt)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80b943c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kamu: apa itu DTSEN?\n",
      "Bot : Maaf, saya tidak menemukan informasi tersebut di dokumen.\n"
     ]
    }
   ],
   "source": [
    "# Contoh pemanggilan sekali saja\n",
    "pertanyaan = \"apa itu DTSEN?\"\n",
    "jawaban = chatbot(pertanyaan)\n",
    "print(\"Kamu:\", pertanyaan)\n",
    "print(\"Bot :\", jawaban)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e94dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Loop Chat ===\n",
    "# while True:\n",
    "#     user_input = input(\"Kamu: \")\n",
    "#     if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "#         break\n",
    "#     print(\"Bot:\", chatbot(user_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fd874d",
   "metadata": {},
   "source": [
    "# batas suci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "518d1883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41b4753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Konfigurasi ===\n",
    "FOLDER_DOCS = \"../bahan-chatbot/txt/\"\n",
    "INDEX_PATH = \"vectorstore_index\"\n",
    "META_PATH = \"index_meta.json\"\n",
    "THRESHOLD = 0.8  # makin kecil = makin ketat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c4dfbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model embedding\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "# Model chatbot\n",
    "model = ChatOllama(model=\"gemma:2b\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acc06516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fungsi baca metadata dokumen ===\n",
    "def get_docs_metadata():\n",
    "    metadata = {}\n",
    "    for filename in os.listdir(FOLDER_DOCS):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            path = os.path.join(FOLDER_DOCS, filename)\n",
    "            metadata[filename] = os.path.getsize(path)  # bisa diganti last modified time\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f90fa0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fungsi membuat index FAISS ===\n",
    "def build_vectorstore():\n",
    "    docs = []\n",
    "    print(f\"üìÇ Membaca dokumen dari: {FOLDER_DOCS}\")\n",
    "    files = [f for f in os.listdir(FOLDER_DOCS) if f.endswith(\".txt\")]\n",
    "\n",
    "    for filename in tqdm(files, desc=\"üìÑ Membaca file TXT\", unit=\"file\"):\n",
    "        with open(os.path.join(FOLDER_DOCS, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            docs.append(Document(page_content=text, metadata={\"source\": filename}))\n",
    "\n",
    "    print(\"\\n‚úÇÔ∏è  Memotong dokumen menjadi chunks...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "    docs_split = []\n",
    "    for doc in tqdm(docs, desc=\"üîπ Memotong teks\", unit=\"dokumen\"):\n",
    "        chunks = splitter.split_documents([doc])\n",
    "        docs_split.extend(chunks)\n",
    "\n",
    "    print(\"\\nüíæ Membuat FAISS index...\")\n",
    "    vectorstore = FAISS.from_documents(docs_split, embeddings)\n",
    "    vectorstore.save_local(INDEX_PATH)\n",
    "\n",
    "    # Simpan metadata file\n",
    "    with open(META_PATH, \"w\") as f:\n",
    "        json.dump(get_docs_metadata(), f)\n",
    "\n",
    "    print(\"‚úÖ Index berhasil dibuat & disimpan!\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6645a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cek apakah perlu rebuild index ===\n",
    "def should_rebuild():\n",
    "    if not os.path.exists(INDEX_PATH) or not os.path.exists(META_PATH):\n",
    "        return True\n",
    "    try:\n",
    "        with open(META_PATH, \"r\") as f:\n",
    "            old_meta = json.load(f)\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "    new_meta = get_docs_metadata()\n",
    "    return old_meta != new_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079825db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load atau rebuild index ===\n",
    "if should_rebuild():\n",
    "    print(\"üîÑ Perubahan dokumen terdeteksi. Membuat index baru...\")\n",
    "    vectorstore = build_vectorstore()\n",
    "else:\n",
    "    print(f\"‚ö° Memuat FAISS index dari {INDEX_PATH}...\")\n",
    "    vectorstore = FAISS.load_local(INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(\"‚úÖ Index berhasil dimuat!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4eb1cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fungsi chatbot ===\n",
    "def chatbot(query):\n",
    "    results = vectorstore.similarity_search_with_score(query, k=3)\n",
    "    relevant_texts = [r[0].page_content for r in results if r[1] < THRESHOLD]\n",
    "\n",
    "    if not relevant_texts:\n",
    "        return \"Maaf, saya tidak menemukan informasi tersebut di dokumen.\"\n",
    "\n",
    "    context = \"\\n\".join(relevant_texts)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "            Jawablah pertanyaan berikut **hanya** berdasarkan informasi dari KONTEN di bawah ini.\n",
    "            Kamu sangat paham terhadap hal-hal yang berkaitan tentang DTSEN (Data Tunggal Sosial Ekonomi Nasional), Kemiskinan, Bantuan Sosial, dan hal terkait itu.\n",
    "            Jika jawabannya tidak ada di KONTEN, jawab: \"Maaf, saya tidak menemukan informasi tersebut di dokumen.\"\n",
    "            Gunakan bahasa Indonesia yang jelas.\n",
    "\n",
    "            KONTEN:\n",
    "            {context}\n",
    "\n",
    "            PERTANYAAN:\n",
    "            {query}\n",
    "            \"\"\"\n",
    "    response = model([HumanMessage(content=prompt)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df870378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kamu: apa itu DTSEN?\n",
      "Bot : Maaf, saya tidak menemukan informasi tersebut di dokumen.\n"
     ]
    }
   ],
   "source": [
    "# === Contoh penggunaan di Notebook ===\n",
    "pertanyaan = \"apa itu DTSEN?\"\n",
    "jawaban = chatbot(pertanyaan)\n",
    "print(\"Kamu:\", pertanyaan)\n",
    "print(\"Bot :\", jawaban)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca011c4d",
   "metadata": {},
   "source": [
    "# batas suci 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adf541b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3de68e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Konfigurasi ===\n",
    "FOLDER_DOCS = \"../bahan-chatbot/txt/\"\n",
    "INDEX_PATH = \"vectorstore_index\"\n",
    "META_PATH = \"index_meta.json\"\n",
    "THRESHOLD = 1  # makin kecil = makin ketat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5564b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model embedding & chatbot\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "model = ChatOllama(model=\"gemma:2b\", temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aecb2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fungsi metadata dokumen ===\n",
    "def get_docs_metadata():\n",
    "    return {\n",
    "        f: os.path.getsize(os.path.join(FOLDER_DOCS, f))\n",
    "        for f in os.listdir(FOLDER_DOCS)\n",
    "        if f.endswith(\".txt\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69d82269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Split dokumen ===\n",
    "def split_documents(files):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    docs_split = []\n",
    "    for filename in tqdm(files, desc=\"üìÑ Memproses dokumen\", unit=\"file\"):\n",
    "        with open(os.path.join(FOLDER_DOCS, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            chunks = splitter.split_text(text)\n",
    "            docs_split.extend([Document(page_content=chunk, metadata={\"source\": filename}) for chunk in chunks])\n",
    "    return docs_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5efd1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Rebuild total index ===\n",
    "def rebuild_index():\n",
    "    files = [f for f in os.listdir(FOLDER_DOCS) if f.endswith(\".txt\")]\n",
    "    print(f\"\\n‚ôªÔ∏è Mendeteksi {len(files)} file. Membuat ulang FAISS index...\")\n",
    "    docs_split = split_documents(files)\n",
    "    print(\"\\nüíæ Menyimpan index baru...\")\n",
    "    vectorstore = FAISS.from_documents(docs_split, embeddings)\n",
    "    vectorstore.save_local(INDEX_PATH)\n",
    "    with open(META_PATH, \"w\") as f:\n",
    "        json.dump(get_docs_metadata(), f)\n",
    "    print(\"‚úÖ Index berhasil dibuat ulang!\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "341d6bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Append file baru ===\n",
    "def append_new_files(vectorstore):\n",
    "    with open(META_PATH, \"r\") as f:\n",
    "        old_meta = json.load(f)\n",
    "    new_meta = get_docs_metadata()\n",
    "    new_files = [f for f in new_meta if f not in old_meta]\n",
    "    if not new_files:\n",
    "        print(\"‚úÖ Tidak ada file baru untuk ditambahkan.\")\n",
    "        return vectorstore\n",
    "    print(f\"\\nüìÇ Menambahkan {len(new_files)} file baru ke index...\")\n",
    "    docs_baru = split_documents(new_files)\n",
    "    print(\"\\nüíæ Menyimpan index yang telah diperbarui...\")\n",
    "    vectorstore.add_documents(docs_baru)\n",
    "    vectorstore.save_local(INDEX_PATH)\n",
    "    old_meta.update({f: new_meta[f] for f in new_files})\n",
    "    with open(META_PATH, \"w\") as f:\n",
    "        json.dump(old_meta, f)\n",
    "    print(\"‚úÖ File baru berhasil ditambahkan.\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b257b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Deteksi perubahan ===\n",
    "def load_or_update_index():\n",
    "    if not os.path.exists(INDEX_PATH) or not os.path.exists(META_PATH):\n",
    "        print(\"üÜï Index belum ada. Membuat baru...\")\n",
    "        return rebuild_index()\n",
    "\n",
    "    with open(META_PATH, \"r\") as f:\n",
    "        old_meta = json.load(f)\n",
    "    new_meta = get_docs_metadata()\n",
    "\n",
    "    # Cek apakah ada file lama berubah\n",
    "    for file, size in new_meta.items():\n",
    "        if file in old_meta and old_meta[file] != size:\n",
    "            print(f\"‚ôªÔ∏è Perubahan terdeteksi di file: {file}\")\n",
    "            return rebuild_index()\n",
    "\n",
    "    # Load index lama\n",
    "    print(f\"‚ö° Memuat FAISS index dari {INDEX_PATH}...\")\n",
    "    vectorstore = FAISS.load_local(INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "    # Tambahkan file baru kalau ada\n",
    "    return append_new_files(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e06bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fungsi chatbot ===\n",
    "def chatbot(query):\n",
    "    results = vectorstore.similarity_search_with_score(query, k=3)\n",
    "    relevant_texts = [r[0].page_content for r in results if r[1] < THRESHOLD]\n",
    "    if not relevant_texts:\n",
    "        return \"Maaf, saya tidak menemukan informasi tersebut di dokumen.\"\n",
    "    context = \"\\n\".join(relevant_texts)\n",
    "    prompt = f\"\"\"\n",
    "Jawablah pertanyaan berikut **hanya** berdasarkan informasi dari KONTEN di bawah ini.\n",
    "Kamu harus memposisikan diri sebagai suatu customer service, jadi harus ramah kepada penanya. Kalau ada yang menyapa ada minta tolong harus kamu bantu.\n",
    "Kamu juga paham banyak hal tentang kementerian sosial dan badan pusat statistik yang ada di Indonesia.\n",
    "Jika jawabannya tidak ada di KONTEN, jawab: \"Maaf, saya tidak menemukan informasi tersebut di dokumen.\"\n",
    "Gunakan bahasa Indonesia yang jelas.\n",
    "\n",
    "KONTEN:\n",
    "{context}\n",
    "\n",
    "PERTANYAAN:\n",
    "{query}\n",
    "\"\"\"\n",
    "    response = model([HumanMessage(content=prompt)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f0dd62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï Index belum ada. Membuat baru...\n",
      "\n",
      "‚ôªÔ∏è Mendeteksi 14 file. Membuat ulang FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìÑ Memproses dokumen: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 559.52file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Menyimpan index baru...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Index berhasil dibuat ulang!\n"
     ]
    }
   ],
   "source": [
    "# === Main ===\n",
    "vectorstore = load_or_update_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e3d211f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kamu: apa itu DTSEN?\n",
      "Bot : Maaf, saya tidak menemukan informasi tersebut di dokumen.\n"
     ]
    }
   ],
   "source": [
    "pertanyaan = \"apa itu DTSEN?\"\n",
    "print(\"Kamu:\", pertanyaan)\n",
    "print(\"Bot :\", chatbot(pertanyaan))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b880bc24",
   "metadata": {},
   "source": [
    "# reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68780e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# INDEX_PATH = \"vectorstore_index\"\n",
    "# META_PATH = \"index_meta.json\"\n",
    "\n",
    "# # Hapus folder FAISS index\n",
    "# if os.path.exists(INDEX_PATH):\n",
    "#     shutil.rmtree(INDEX_PATH)\n",
    "#     print(f\"‚úÖ Folder {INDEX_PATH} dihapus.\")\n",
    "\n",
    "# # Hapus metadata\n",
    "# if os.path.exists(META_PATH):\n",
    "#     os.remove(META_PATH)\n",
    "#     print(f\"‚úÖ File {META_PATH} dihapus.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
